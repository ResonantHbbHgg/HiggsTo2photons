#! /usr/bin/python
import sys, os, re
import datetime, time
import shlex as _shlex
import subprocess as _subprocess
import random

def source_grid():
    if (os.getenv("GLITE_ENV_SET") is None):
        os.environ["GLITE_ENV_SET"] = "MATTEO"
        os.environ["PYTHONPATH"] = "/code/osgcode/ucsdt2/gLite/lcg/lib/python"
        os.environ["PYTHONPATH"] = "/code/osgcode/ucsdt2/gLite/external/opt/ZSI/lib/python2.3/site-packages"
        os.environ["PYTHONPATH"] = "/code/osgcode/ucsdt2/gLite/external/opt/SOAPpy/lib/python2.3/site-packages"
        os.environ["PYTHONPATH"] = "/code/osgcode/ucsdt2/gLite/external/opt/fpconst/lib/python2.3/site-packages"
        os.environ["PERL5LIB"] = "/code/osgcode/ucsdt2/gLite/lcg/lib/perl"
        os.environ["PERL5LIB"] = "/code/osgcode/ucsdt2/gLite/external/usr/lib/perl5/vendor_perl/5.8.5"
        os.environ["LD_LIBRARY_PATH"] ="/code/osgcode/ucsdt2/gLite/external/usr/lib"
        os.environ["LD_LIBRARY_PATH"] = "/code/osgcode/ucsdt2/gLite/external/opt/c-ares/lib"
        
        os.environ["X509_VOMS_DIR"] = "/code/osgcode/ucsdt2/gLite/external/etc/grid-security/vomsdir"
        os.environ["X509_CERT_DIR"] = "/etc/grid-security/certificates"
        os.environ["SRM_PATH"] = "/code/osgcode/ucsdt2/gLite/d-cache/srm"
        os.environ["RGMA_HOME"] = "/code/osgcode/ucsdt2/gLite/glite"
        os.environ["MYPROXY_TCP_PORT_RANGE"] = "20000,25000"
        os.environ["MYPROXY_SERVER"] = "myproxy.cern.ch"
        os.environ["LCG_LOCATION"] = "/code/osgcode/ucsdt2/gLite/lcg"
        os.environ["LCG_GFAL_INFOSYS"] = "uscmsbd2.fnal.gov:2170"
        if (os.getenv("X509_USER_PROXY") is None):
            os.environ["X509_USER_PROXY"] = "/tmp/x509up_u$(id -u)"
        if (os.getenv("JAVA_HOME") is None):
            os.environ["JAVA_HOME"] = "/opt/vdt/jdk1.5"
            
        os.environ["GT_PROXY_MODE"] = "old"
        os.environ["GRID_ENV_LOCATION"] = "/code/osgcode/ucsdt2/gLite/external/etc/profile.d"
        os.environ["GLOBUS_TCP_PORT_RANGE"] = "20000,25000"
        os.environ["GLOBUS_LOCATION"] = "/code/osgcode/ucsdt2/gLite/globus"
        os.environ["GLITE_WMS_LOCATION"] = "/code/osgcode/ucsdt2/gLite/glite"
        os.environ["GLITE_SD_PLUGIN"] = "bdii,rgma"
        os.environ["GLITE_LOCATION_VAR"] = "/code/osgcode/ucsdt2/gLite/glite/var"
        os.environ["GLITE_LOCATION"] = "/code/osgcode/ucsdt2/gLite/glite"
        os.environ["EDG_WL_LOCATION"] = "/code/osgcode/ucsdt2/gLite/edg"
        os.environ["EDG_LOCATION"] = "/code/osgcode/ucsdt2/gLite/edg"
        os.environ["APEL_HOME"] = "/code/osgcode/ucsdt2/gLite/glite"

        os.environ["PYTHONPATH"] += ":/code/osgcode/ucsdt2/gLite/glite/lib/python"
        os.environ["PYTHONPATH"] += ":/code/osgcode/ucsdt2/gLite/glite/lib"
        os.environ["PYTHONPATH"] += ":/code/osgcode/ucsdt2/gLite/glite/lib"
        os.environ["PATH"] += ":/usr/bin"
        os.environ["PATH"] += ":/code/osgcode/ucsdt2/jdk1.5/bin"
        os.environ["PATH"] += ":/code/osgcode/ucsdt2/gLite/lcg/bin"
        os.environ["PATH"] += ":/code/osgcode/ucsdt2/gLite/globus/bin"
        os.environ["PATH"] += ":/code/osgcode/ucsdt2/gLite/glite/bin"
        os.environ["PATH"] += ":/code/osgcode/ucsdt2/gLite/edg/bin"
        os.environ["PATH"] += ":/code/osgcode/ucsdt2/gLite/d-cache/srm/bin:/code/osgcode/ucsdt2/gLite/d-cache/dcap/bin"
        if (os.getenv("MANPATH") is None):
            os.environ["MANPATH"] = "/opt/glite/yaim/man"
        else:
            os.environ["MANPATH"] += ":/opt/glite/yaim/man"

        os.environ["MANPATH"] += ":/code/osgcode/ucsdt2/gLite/lcg/man"
        os.environ["MANPATH"] += ":/code/osgcode/ucsdt2/gLite/globus/man"
        os.environ["MANPATH"] += ":/code/osgcode/ucsdt2/gLite/glite/share/man"
        os.environ["MANPATH"] += ":/code/osgcode/ucsdt2/gLite/edg/share/man"
        os.environ["LD_LIBRARY_PATH"] += ":/code/osgcode/ucsdt2/gLite/lcg/lib"
        os.environ["LD_LIBRARY_PATH"] += ":/code/osgcode/ucsdt2/gLite/globus/lib"
        os.environ["LD_LIBRARY_PATH"] += ":/code/osgcode/ucsdt2/gLite/glite/lib"
        os.environ["LD_LIBRARY_PATH"] += ":/code/osgcode/ucsdt2/gLite/external/opt/xerces-c/lib/"
        os.environ["LD_LIBRARY_PATH"] += ":/code/osgcode/ucsdt2/gLite/external/opt/log4cxx/lib/"
        os.environ["LD_LIBRARY_PATH"] += ":/code/osgcode/ucsdt2/gLite/d-cache/dcap/lib"
       

def pipe(cmdline, input = None):
  """
  wrapper around subprocess to simplify te interface
  """
  args = _shlex.split(cmdline)

  if input is not None:
    command = _subprocess.Popen(args, stdin = _subprocess.PIPE, stdout = _subprocess.PIPE, stderr = None)
  else:
    command = _subprocess.Popen(args, stdin = None, stdout = _subprocess.PIPE, stderr = None)

  (out, err) = command.communicate(input)
  return out

def check_dir():
    if (os.path.basename(os.getcwd) != "reduce"):
        print "error - must run script from the reduce directory.  aborting"
    sys.exit(-3)


if __name__ == "__main__":

    # check host
    if ("uaf" not in os.getenv("HOSTNAME")):
        print "Sorry but the script is done to be run on UAF machines at UCSD."
        sys.exit(-1)
    
    top = os.getcwd()
    temp = ""
    temp1 = pipe("scram tool info root", temp)
    temp2 = pipe("grep Version", temp1)
    rootVersion = pipe("awk '{print $3}'", temp2).split("\n")[0]
    CMSSWVersion  = re.search("(CMSSW.+)/s", top).group(1)
    username      = os.getlogin()
    hadoop_prefix = "srm://bsrm-1.t2.ucsd.edu:8443/srm/v2/server?SFN="
    hadoop_home   = "/hadoop/cms/store/user/" + username

    # reduce_batch input ---
    REQUIRED_ARGS = 9
    
    if (len(sys.argv) != REQUIRED_ARGS):
        print "input ${INPUT_ARGS} arguments, ${REQUIRED_ARGS} required -- aborting\n"
        print "Usage: is ./reduce_batch arg1 arg2 arg3 arg4 arg5 arg6 arg7 arg8\n"
        print "arg1: tag \t\t\t\t\t(jets, hww160, ttbar, ...)"
        print "arg2: analysisname \t\t\t\t(PhotonAnalysis, ...)"
        print "arg3: type \t\t\t\t(for reducing differently for different data)"
        print "arg4: root_seed \t\t\t(the beginning of your filenames to run on)"
        print "arg5: hadoop_data_directory \t\t( /hadoop/cms/store/user/ . . . )"
        print "arg6: dat file\t\t\t"
        print "arg7: inputBranches file\t\t\t"
        print "arg8: outputBranches file\t\t\t"
        sys.exit(-4)
    print "\nPreparing batch reduction...\n"
    
    tag = sys.argv[1]
    analysisname = sys.argv[2]
    itype = sys.argv[3]
    rootseed = sys.argv[4]
    thedatadirectory = os.path.dirname(sys.argv[5]) + "/"+ os.path.basename(sys.argv[5])
    datfile = sys.argv[6]
    inputBranchesFile = sys.argv[7]
    outputBranchesFile = sys.argv[8]
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    outputdir = timestamp + "_" + tag
    theglobedirectory = os.path.dirname(os.getcwd())
    
    os.system("rm -rf reduce")
    os.system("mkdir reduce")
    
    #**********************************************************************
    os.system("ls -1 "+ thedatadirectory + "/" + rootseed + "*.root | grep -v _CMSSW_ > reduce/allfilenames.txt")
    
    #**********************************************************************
    totalfiles = pipe("wc -l reduce/allfilenames.txt")
    totalfiles = int(totalfiles.split(" ")[0])
    if (totalfiles == 0):
        print "no files ??"
        sys.exit(-5)
    
    #*************************************************************************************
    totalsizelimit = 10000000000
    
    a,b,c = os.popen3("ls -l " + thedatadirectory + "/" + rootseed + "*.root")
    temp = ""
    for i in b.readlines():
        temp += i
    totalfilesize = int(pipe("awk \'{sum+=$5}; END{print sum}\'", temp))
    
    numberofjobs = totalfilesize/totalsizelimit + 1
    avgfilesize  = totalfilesize/totalfiles
    filesperjob  = totalfiles/numberofjobs + 1
    if (filesperjob > 500):
        print "\ntoo many files per job: " + str(filesperjob) + ".  Limiting to 500."
        filesperjob=500
        numberofjobs = totalfiles/500 + 1
    
    filesinlastjob = totalfiles - filesperjob *(numberofjobs - 1)
    if ((filesinlastjob < 0) == 1):
        numberofjobs   = numberofjobs + filesinlastjob/filesperjob - 1
        filesinlastjob = totalfiles - filesperjob*(numberofjobs - 1)
    
    tenG = 10000000000
    tenM = 10000000
    tenk = 10000
    
    totalfilesizeprint = totalfilesize
    if (totalfilesizeprint > tenG):
        totalfilesizeprint = totalfilesizeprint/(tenG/10)
        totalfilesizeprint = str(totalfilesizeprint) + " G"
    else:
      if (totalfilesizeprint > tenM):
          totalfilesizeprint = totalfilesizeprint/(tenM/10)
          totalfilesizeprint = str(totalfilesizeprint) + " M"
      else:
        if (totalfilesizeprint > tenk):
            totalfilesizeprint = totalfilesizeprint/(tenk/10)
            totalfilesizeprint = str(totalfilesizeprint) + " k"
    
    avgfilesizeprint = avgfilesize
    if (avgfilesizeprint > tenG):
        avgfilesizeprint = avgfilesizeprint/(tenG/10)
        avgfilesizeprint = str(avgfilesizeprint) + " G"
    else:
      if (avgfilesizeprint > tenM):
          avgfilesizeprint = avgfilesizeprint/(tenM/10)
          avgfilesizeprint = str(avgfilesizeprint) + " M"
      else:
        if (avgfilesizeprint > tenk):
            avgfilesizeprint = avgfilesizeprint/(tenk/10)
            avgfilesizeprint = str(avgfilesizeprint) + " k"
    
    print "\ntotalfiles:     ", totalfiles
    print "totalfilesize:  ", totalfilesizeprint
    print "avgfilesize:    ", avgfilesizeprint
    print "numberofjobs:   ", numberofjobs
    print "filesperjob:    ", filesperjob
    print "filesinlastjob: ", filesinlastjob,"\n"
    print "reducing " + str(totalfiles) +" files in " + str(numberofjobs) + " jobs (avg file size: "+ str(avgfilesize) +")."
    
    #*************************************************************************************
    
    source_grid()
    a,b,c = os.popen3("voms-proxy-info 2>&1")
    temp = ""
    for i in b.readlines():
        temp += i
    temp1 = pipe("grep time", temp)
    temp2 = pipe("awk -F: \'{if($2 > 30) print \"proxy ok\"}\'", temp1)
    temp3 = pipe("grep \"proxy ok\"", temp2)
    if (temp3 != "proxy ok\n"):
        os.system("voms-proxy-destroy")
        os.system("voms-proxy-init -valid 36:00")
    
    file = open("reduce/allfilenames.txt")
    file_names = file.readlines()
    file.close()
    
    outputfilename = rootseed + "_red_" + tag + "_typ" + str(itype)
    numberofjobs = 0
    
    for f in xrange(0, len(file_names), filesperjob):
        numberofjobs += 1
        limits = (f, filesperjob+f)
        if (limits[1] > len(file_names)):
            limits = (f, len(file_names))
        
        #inp = open(analysisname+"_scripts/"+templatefile, "r")
        #lines = inp.readlines()
        #inp.close()
    
        datafilename = rootseed + "_datafiles"+ str(numberofjobs) + ".dat"
        txtfilename = rootseed + "_datafiles"+ str(numberofjobs) + ".txt"
        
        out = open("reduce/"+datafilename, "w")
        out2 = open("reduce/"+txtfilename, "w")
        out.write("output=" + outputfilename + "_job" + str(numberofjobs) +".root\n")
        for i in file_names[limits[0]:limits[1]]:
            out.write("Fil=" + i.split("\n")[0] + " typ=" + itype + "\n")
            out2.write(i)
            out.write("\n")    
        out.write("inputBranches " + inputBranchesFile + "\n")
        out.write("outputBranches " + outputBranchesFile + "\n")
        out.write("analyzer " + analysisname + " " + datfile + "\n")
                
        out.close()
        out2.close()
    
    os.system("cp -r AnalysisScripts/* reduce/.")
    
    thetarball = "reduce" + timestamp + ".tgz"
    os.system("tar zcf " + thetarball + " reduce libLoopAll.so")
    
    if (not os.path.exists(hadoop_home + "/reduced")):
        os.mkdir(hadoop_home + "/reduced")
    
    if (not os.path.exists(hadoop_home + "/condorfiles")):
        os.mkdir(hadoop_home + "/condorfiles")
    
    os.system("cp " + thetarball + " " + hadoop_home + "/condorfiles/")
    os.system("rm " + thetarball)
    
    for ijob in xrange(1, numberofjobs):
        datafilename = rootseed + "_datafiles" + str(ijob) + ".dat"
        os.system("rm -f " + datafilename)
    
    os.system("rm -f reduce/allfilenames.txt")
    condordir = "/home/users/" + username + "/condorfiles/" + timestamp + "_red_" + rootseed
    
    os.mkdir(hadoop_home + "/reduced/" + outputdir)
    
    if (not os.path.exists("/home/users/" + username + "/condorfiles")):
        os.mkdir("/home/users/" + username + "/condorfiles")
        
    if (not os.path.exists(condordir)):
        os.mkdir(condordir)
    
    os.chdir(condordir)
    os.mkdir("output")
    
    for ijob in xrange(1, numberofjobs+1):
        jobname = "reduce_"+ rootseed + "_job" + str(ijob)
        thesourcedir = username + timestamp + "job" + str(ijob)
        datafilename = rootseed + "_datafiles" + str(ijob) + ".dat"
        txtfilename = rootseed + "_datafiles"+ str(numberofjobs) + ".txt"
           
        file = open(jobname + ".bash", "w")
        file.write("#!/bin/bash\n")
        file.write("umask 002\n")
        file.write("/bin/hostname\n")
        file.write("echo \"Who am I?\"\n")
        file.write("/usr/bin/whoami\n")
        file.write("\n")
        file.write("echo \"OSG_GRID: \"\n")
        file.write("echo ${OSG_GRID}\n")
        file.write("echo \"df\"\n")
        file.write("df\n")
        file.write("\n")
        file.write("## setup env\n")
        file.write("export PATH=/usr/local/bin:/bin:/usr/bin\n")
        file.write("export SCRAM_ARCH=slc5_amd64_gcc462\n")
        file.write("source $OSG_APP/cmssoft/cms/cmsset_default.sh\n")
        file.write("cd $OSG_APP/cmssoft/cms/slc5_amd64_gcc462/cms/cmssw/" + CMSSWVersion + "/src\n")
        file.write("eval \`scram runtime -sh\`\n")
        file.write("export SRM_PATH=$OSG_APP/ucsdt2/gLite/d-cache/srm\n")
        file.write("export PATH=$PATH:$OSG_APP/ucsdt2/gLite/d-cache/srm/bin\n")
        file.write("export JAVA_HOME=/usr/java/latest\n")
        
        file.write("cd -\n")
        file.write("\n")
        file.write("landdir=\"$PWD\"\n")
        file.write("sourcedir=\"$PWD/%s\"\n" %(thesourcedir))
        file.write("\n")
        file.write("mkdir globe\n")
        file.write("cd globe\n")
        file.write("cp %s/condorfiles/%s .\n" % (hadoop_home, thetarball))
        
        file.write("tar -zxvf %s\n" % (thetarball))
        file.write("rm %s\n" % (thetarball) )
        file.write("cd reduce\n")
        file.write("mkdir ${sourcedir}\n")
        file.write("\n")
    
        file.write("./reduce.py -i %s\n" % (datafilename))
        file.write("echo \"landir\"\n")
        file.write("echo ${landdir}\n")
        file.write("\n")
        file.write("ls -1 ${landdir}/globe/reduce/*.root > theoutputfiles.txt\n")
    
        file.write("\n")
        file.write("echo \"****************** catting theoutputfiles.txt ****************\"\n")
        file.write("cat theoutputfiles.txt\n")
        file.write("echo \"****************** catting theoutputfiles.txt ****************\"\n")
        file.write("\n")
    
        file.write("for ifile in `/usr/bin/less theoutputfiles.txt`;do\n")
        file.write("barefilename=`basename ${ifile}`\n")
        file.write("\n")
        file.write("srmcp -2 -debug=true -delegate=false file:////${ifile} %s%s/reduced/%s/${barefilename} > srmcpfile.txt 2>&1 \n" % (hadoop_prefix, hadoop_home, outputdir))
        file.write("/usr/bin/less srmcpfile.txt\n")
        file.write("\n")
        file.write("if [ ! `grep -i fail srmcpfile.txt  | awk '{print NR}'| /usr/bin/tail -1` ];then \n")
        file.write("    echo \"DONE\"\n")
        file.write("else\n")
        file.write("   echo \"srmcp failed - attempting globus-url-copy to home area of %s\"; \n" % (username))
        file.write("   globus-url-copy file:///${ifile} gsiftp://uaf-2.t2.ucsd.edu/home/users/%s/${barefilename} > globusurlcopy.txt\n" % (username))
        file.write("   if [ ! `less globusurlcopy.txt | awk '{print NR}'|tail -1` ]; then\n")
        file.write("      echo \"DONE\"\n")
        file.write("   else\n")
        file.write("     echo \"globus-url-copy failed also - losing output, sorry - updating database with status 0\"; \n")
        file.write("   fi\n")
        file.write("fi\n")
        file.write("\n")
        file.write("\n")
        file.write("done\n")
        file.write("\n")
        file.write("cd ../..\n")
        file.write("echo \"clean up -------------------------------------\"\n")
        file.write("rm -rf globe\n")
        file.write("rm -rf " + thesourcedir + "\n")
        file.write("exit\n")
        file.close()
        
        file = open(jobname + ".sub", "w")
        text = """
    universe=grid
    Grid_Resource=gt2 osg-gw-5.t2.ucsd.edu:/jobmanager-condor
    executable=%s.bash
    stream_output = False
    stream_error  = False
    WhenToTransferOutput = ON_EXIT
    transfer_input_files =
    transfer_Output_files =
    log    = %s/test%s.log
    Notification = Never
    
    
    output = ./output/condor_batch.$(Cluster).$(Process).out
    error  = ./output/condor_batch.$(Cluster).$(Process).err
    queue
    """ % (jobname, condordir, str(ijob))
        file.write(text)
        file.close()
        
        print "\nsubmitting " + jobname + ".sub\n"
        os.system("condor_submit " + jobname + ".sub")
        time.sleep(1)
    
