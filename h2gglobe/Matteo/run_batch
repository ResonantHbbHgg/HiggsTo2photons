#! /usr/bin/python
import sys, os, re
import datetime, time
import shlex as _shlex
import subprocess as _subprocess

def source(script, update=1):
    p = _subprocess.Popen(". %s; env" % script, stdout=_subprocess.PIPE, shell=True)
    data = p.communicate()[0]

    env = dict((line.split("=", 1) for line in data.splitlines()))
    if update:
        os.environ.update(env)

def pipe(cmdline, input = None):
  """
  wrapper around subprocess to simplify te interface
  """
  args = _shlex.split(cmdline)

  if input is not None:
    command = _subprocess.Popen(args, stdin = _subprocess.PIPE, stdout = _subprocess.PIPE, stderr = None)
  else:
    command = _subprocess.Popen(args, stdin = None, stdout = _subprocess.PIPE, stderr = None)

  (out, err) = command.communicate(input)
  return out

if __name__ == "__main__":

    # check host
    if ("uaf" not in os.getenv("HOSTNAME")):
        print "Sorry but the script is done to be run on UAF machines at UCSD."
        sys.exit(-1)
    
    top = os.getcwd()
    temp = ""
    temp1 = pipe("scram tool info root", temp)
    temp2 = pipe("grep Version", temp1)
    rootVersion = pipe("awk '{print $3}'", temp2).split("\n")[0]
    CMSSWVersion  = re.search("(CMSSW.+)/s", top).group(1)
    username      = os.getlogin()
    hadoop_prefix = "srm://bsrm-1.t2.ucsd.edu:8443/srm/v2/server?SFN="
    hadoop_home   = "/hadoop/cms/store/user/" + username

    # reduce_batch input ---
    REQUIRED_ARGS = 5
    
    if (len(sys.argv) != REQUIRED_ARGS):
        print "input ${INPUT_ARGS} arguments, ${REQUIRED_ARGS} required -- aborting\n"
        print "Usage: is ./reduce_batch arg1 arg2 arg3 arg4\n"
        print "arg1: dat file"
        print "arg2: dir file"
        print "arg3: num jobs"
        print "arg4: label"
        sys.exit(-4)
    print "\nPreparing batch reduction...\n"
    
    datfile = "AnalysisScripts/"+sys.argv[1]
    dirfile = sys.argv[2]
    numberofjobs = int(sys.argv[3])
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    outputdir = timestamp + "_" + sys.argv[4]
    theglobedirectory = os.path.dirname(os.getcwd())
    
    os.system("rm -rf process")
    os.system("mkdir process")
    
    #**********************************************************************
    dir_file = open(dirfile, "r")
    lines = dir_file.readlines()
    dir_file.close()

    out_file = open("process/allfilenames.txt", "w")
    
    for l in lines:
        if (l.startswith("typ")):
            if ("Dir" in l):
                params = l.split()
                pippo = os.popen("ls -1 "+ params[4][4:] + "/*.root").readlines()
                for i in pippo:
                    params[4] = "Fil="+i.split("\n")[0]
                    out_file.write(" ".join(params)+"\n")

    out_file.close()
    
    #**********************************************************************
    totalfiles = pipe("wc -l process/allfilenames.txt")
    totalfiles = int(totalfiles.split(" ")[0])
    if (totalfiles == 0):
        print "no files ??"
        sys.exit(-5)

    filesperjob = totalfiles/numberofjobs + 1
    
    #*************************************************************************************
    
    source("/code/osgcode/ucsdt2/gLite32/etc/profile.d/grid_env.sh")
    os.environ["LCG_GFAL_INFOSYS"] = "lcg-bdii.cern.ch:2170"
    os.environ["GLOBUS_TCP_PORT_RANGE"] = "20000,25000"
    source("/code/osgcode/ucsdt2/Crab/etc/crab.sh")
               
    a,b,c = os.popen3("voms-proxy-info 2>&1")
    temp = ""
    for i in b.readlines():
        temp += i
    temp1 = pipe("grep time", temp)
    temp2 = pipe("awk -F: \'{if($2 > 30) print \"proxy ok\"}\'", temp1)
    temp3 = pipe("grep \"proxy ok\"", temp2)
    if (temp3 != "proxy ok\n"):
        os.system("voms-proxy-destroy")
        os.system("voms-proxy-init -valid 36:00")
    
    file = open("process/allfilenames.txt")
    file_names = file.readlines()
    file.close()

    numberofjobs = 0
    
    for f in xrange(0, len(file_names), filesperjob):
        numberofjobs += 1
        limits = (f, filesperjob+f)
        if (limits[1] > len(file_names)):
            limits = (f, len(file_names))
    
        datafilename = "datafiles"+ str(numberofjobs) + ".dat"
        out = open("process/" + datafilename, "w")

        inputdat = open(datfile, "r")
        lines = inputdat.readlines()
        inputdat.close()
        
        for l in lines:
            if ("histfile" in l):
                l = l.replace(".root", "_"+str(numberofjobs)+".root")
                l = l.replace(".txt", "_"+str(numberofjobs)+".txt")
            out.write(l)
        out.write("\n") 
        
        for i in file_names[limits[0]:limits[1]]:
            out.write(i + "\n")
        out.write("\n")    
        out.close()
    
    os.system("cp -r AnalysisScripts/* process/.")
    thetarball = "process" + timestamp + ".tgz"
    os.system("cd ../..;tar zcf " + thetarball + " src/h2gglobe/process src/h2gglobe/libLoopAll.so  src/EGamma src/CMGTools")
    
    if (not os.path.exists(hadoop_home + "/processed")):
        os.mkdir(hadoop_home + "/processed")
    
    if (not os.path.exists(hadoop_home + "/condorfiles")):
        os.mkdir(hadoop_home + "/condorfiles")
    
    os.system("cp ../../" + thetarball + " " + hadoop_home + "/condorfiles/")
    os.system("rm ../../" + thetarball)
    
    for ijob in xrange(1, numberofjobs):
        datafilename = "datafiles" + str(ijob) + ".dat"
        os.system("rm -f " + datafilename)
    
    os.system("rm -f process/allfilenames.txt")
    condordir = "/home/users/" + username + "/condorfiles/" + timestamp + "_pro"
    
    os.mkdir(hadoop_home + "/processed/" + outputdir)
    
    if (not os.path.exists("/home/users/" + username + "/condorfiles")):
        os.mkdir("/home/users/" + username + "/condorfiles")
        
    if (not os.path.exists(condordir)):
        os.mkdir(condordir)
    
    os.chdir(condordir)
    os.mkdir("output")
    
    for ijob in xrange(1, numberofjobs+1):
        jobname = "process_job" + str(ijob)
        thesourcedir = username + timestamp + "job" + str(ijob)
        datafilename = "datafiles" + str(ijob) + ".dat"
           
        file = open(jobname + ".bash", "w")
        file.write("#!/bin/bash\n")
        file.write("umask 002\n")
        file.write("\n")
        file.write("## setup env\n")
        file.write("export PATH=/usr/local/bin:/bin:/usr/bin\n")
        file.write("export SCRAM_ARCH=slc5_amd64_gcc462\n")
        file.write("source $OSG_APP/cmssoft/cms/cmsset_default.sh\n")
        file.write("scram project CMSSW " + CMSSWVersion + "\n")
        file.write("cd " + CMSSWVersion + "/src\n")
        file.write("eval \`scram runtime -sh\`\n")
        file.write("export LD_LIBRARY_PATH=${LD_LIBRARY_PATH#\\\"}\n")
        file.write("export LD_LIBRARY_PATH=${LD_LIBRARY_PATH%\\\";}\n")
        file.write("export PYTHONPATH=${PYTHONPATH#\\\"}\n")
        file.write("export PYTHONPATH=${PYTHONPATH%\\\";}\n")
        file.write("export CMSSW_SEARCH_PATH=${CMSSW_SEARCH_PATH#\\\"}\n")
        file.write("export CMSSW_SEARCH_PATH=${CMSSW_SEARCH_PATH%\\\";}\n")
        file.write("export CMSSW_BASE=${CMSSW_BASE#\\\"}\n")
        file.write("export CMSSW_BASE=${CMSSW_BASE%\\\";}\n")
        file.write("export CMSSW_SEARCH_PATH=$CMSSW_SEARCH_PATH:$CMSSW_BASE/CMGTools/External/data\n")
        file.write("export PATH=${PATH#\\\"}\n")
        file.write("export PATH=${PATH%\\\";}\n")
        file.write("export SRM_PATH=$OSG_APP/ucsdt2/gLite/d-cache/srm\n")
        file.write("export PATH=$PATH:$OSG_APP/ucsdt2/gLite/d-cache/srm/bin\n")
        file.write("export JAVA_HOME=/usr/java/latest\n")
        file.write("landdir=\"$PWD\"\n")
        file.write("sourcedir=\"$PWD/%s\"\n" %(thesourcedir))
        file.write("\n")
        file.write("cd ..\n")
        file.write("cp %s/condorfiles/%s .\n" % (hadoop_home, thetarball))
        
        file.write("tar -zxf %s\n" % (thetarball))
        file.write("rm %s\n" % (thetarball) )
        file.write("cd src/\n")
        file.write("scram b -j 6\n")
        file.write("cd h2gglobe/process\n")
        file.write("mkdir ${sourcedir}\n")
        file.write("\n")
        
        file.write("./fitter.py -i %s\n" % (datafilename))
        file.write("echo \"landir\"\n")
        file.write("echo ${landdir}\n")
        file.write("\n")
        file.write("ls -1 ${landdir}/h2gglobe/process/*.root > theoutputfiles.txt\n")
    
        file.write("\n")
        file.write("echo \"****************** catting theoutputfiles.txt ****************\"\n")
        file.write("cat theoutputfiles.txt\n")
        file.write("echo \"****************** catting theoutputfiles.txt ****************\"\n")
        file.write("\n")
    
        file.write("for ifile in `/usr/bin/less theoutputfiles.txt`;do\n")
        file.write("barefilename=`basename ${ifile}`\n")
        file.write("\n")
        file.write("srmcp -2 -debug=true -delegate=false file:////${ifile} %s%s/processed/%s/${barefilename} > srmcpfile.txt 2>&1 \n" % (hadoop_prefix, hadoop_home, outputdir))
        file.write("/usr/bin/less srmcpfile.txt\n")
        file.write("\n")
        file.write("if [ ! `grep -i fail srmcpfile.txt  | awk '{print NR}'| /usr/bin/tail -1` ];then \n")
        file.write("    echo \"DONE\"\n")
        file.write("else\n")
        file.write("   echo \"srmcp failed - attempting globus-url-copy to home area of %s\"; \n" % (username))
        file.write("   globus-url-copy file:///${ifile} gsiftp://uaf-2.t2.ucsd.edu/home/users/%s/${barefilename} > globusurlcopy.txt\n" % (username))
        file.write("   if [ ! `less globusurlcopy.txt | awk '{print NR}'|tail -1` ]; then\n")
        file.write("      echo \"DONE\"\n")
        file.write("   else\n")
        file.write("     echo \"globus-url-copy failed also - losing output, sorry - updating database with status 0\"; \n")
        file.write("   fi\n")
        file.write("fi\n")
        file.write("\n")
        file.write("\n")
        file.write("done\n")
        file.write("\n")
        file.write("cd ../../../..\n")
        file.write("echo \"clean up -------------------------------------\"\n")
        file.write("rm -rf " + CMSSWVersion + "\n")
        file.write("exit\n")
        file.close()
        
        file = open(jobname + ".sub", "w")
        text = """
    universe=grid
    Grid_Resource=gt2 osg-gw-2.t2.ucsd.edu:/jobmanager-condor
    executable=%s.bash
    stream_output = False
    stream_error  = False
    WhenToTransferOutput = ON_EXIT
    transfer_input_files =
    transfer_Output_files =
    log    = %s/test%s.log
    Notification = Never
    
    
    output = ./output/condor_batch.$(Cluster).$(Process).out
    error  = ./output/condor_batch.$(Cluster).$(Process).err
    queue
    """ % (jobname, condordir, str(ijob))
        file.write(text)
        file.close()
        
        print "\nsubmitting " + jobname + ".sub\n"
        os.system("condor_submit " + jobname + ".sub")
        time.sleep(1)
    
