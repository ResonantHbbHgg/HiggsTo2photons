#! /usr/bin/python
import sys, os, re
import datetime, time
import shlex as _shlex
import subprocess as _subprocess

def source(script, update=1):
    p = _subprocess.Popen(". %s; env" % script, stdout=_subprocess.PIPE, shell=True)
    data = p.communicate()[0]
    env=dict()
    #env = dict((line.split("=", 1) for line in data.splitlines()))
    for line in data.splitlines():
        param = line.split("=", 1)
        if (len(param) > 2):
            env[param[0]]= param[1]

    if update:
        os.environ.update(env)

def pipe(cmdline, input = None):
  """
  wrapper around subprocess to simplify te interface
  """
  args = _shlex.split(cmdline)

  if input is not None:
    command = _subprocess.Popen(args, stdin = _subprocess.PIPE, stdout = _subprocess.PIPE, stderr = None)
  else:
    command = _subprocess.Popen(args, stdin = None, stdout = _subprocess.PIPE, stderr = None)

  (out, err) = command.communicate(input)
  return out

if __name__ == "__main__":

    # check host
    if ("uaf" not in os.getenv("HOSTNAME")):
        print "Sorry but the script is done to be run on UAF machines at UCSD."
        sys.exit(-1)

    temp = ""
    temp1 = pipe("scram tool info root", temp)
    temp2 = pipe("grep Version", temp1)
    rootVersion = pipe("awk '{print $3}'", temp2).split("\n")[0]
    CMSSWVersion  = re.search("(CMSSW.+)/s", os.getcwd()).group(1)
    username      = os.getlogin()
    hadoop_prefix = "srm://bsrm-1.t2.ucsd.edu:8443/srm/v2/server?SFN="
    hadoop_home   = "/hadoop/cms/store/user/" + username
    label = ""
    
    # reduce_batch input ---
    REQUIRED_ARGS = 4
    
    if (len(sys.argv) != REQUIRED_ARGS):
        print "input ${INPUT_ARGS} arguments, ${REQUIRED_ARGS} required -- aborting\n"
        print "Usage: is ./run_batch arg1 arg2 arg3\n"
        print "arg1: dat file (path relative to AnalysisScripts)"
        print "arg2: num jobs"
        print "arg3: label"
        sys.exit(-4)
    print "\nPreparing batch reduction...\n"
   
    datfilename = "AnalysisScripts/"+sys.argv[1]
    numberofjobs = int(sys.argv[2])    
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    outputdir = timestamp + "_" + sys.argv[3]
    label = sys.argv[3]

    basedir = os.path.abspath(sys.argv[0].split("Matteo/run_batch")[0])
    basename = os.path.basename(basedir)
    os.system("cd "+basedir)
    
    os.system("rm -rf " + label)
    os.system("mkdir " + label)
    
    os.system("cd AnalysisScripts")
    os.system("python fitter.py -i "+sys.argv[1]+" --dryRun &> ../"+label+"/dryRun.log")
    os.system("cd ..")
        

    #**********************************************************************
    datfile = open(datfilename, "r")
    lines = datfile.readlines()
    datfile.close()

    #template_file = open("process/template.dat", "w")
    #out_file = open("process/allfilenames.txt", "w")
    
    #for l in lines:
    #    if (l.startswith("typ")):
    #        if ("Dir" in l):
    #            params = l.split()
    #            pippo = os.popen("ls -1 "+ params[4][4:] + "/*.root").readlines()
    #            for i in pippo:
    #                params[4] = "Fil="+i.split("\n")[0]
    #                out_file.write(" ".join(params)+"\n")
    #    else:
    #        template_file.write(l+"\n")
    #
    #template_file.close()
    #out_file.close()
    
    #**********************************************************************
    #totalfiles = pipe("wc -l process/allfilenames.txt")
    #totalfiles = int(totalfiles.split(" ")[0])
    #if (totalfiles == 0):
    #    print "no files ??"
    #    sys.exit(-5)

    #filesperjob = totalfiles/numberofjobs + 1
    
    #*************************************************************************************
    
    source("/code/osgcode/ucsdt2/gLite32/etc/profile.d/grid_env.sh")
    os.environ["LCG_GFAL_INFOSYS"] = "lcg-bdii.cern.ch:2170"
    os.environ["GLOBUS_TCP_PORT_RANGE"] = "20000,25000"
    source("/code/osgcode/ucsdt2/Crab/etc/crab.sh")
               
    a,b,c = os.popen3("voms-proxy-info 2>&1")
    temp = ""
    for i in b.readlines():
        temp += i
    temp1 = pipe("grep time", temp)
    temp2 = pipe("awk -F: \'{if($2 > 30) print \"proxy ok\"}\'", temp1)
    temp3 = pipe("grep \"proxy ok\"", temp2)
    if (temp3 != "proxy ok\n"):
        os.system("voms-proxy-destroy")
        os.system("voms-proxy-init -valid 36:00")
    
    #file = open(label+"/allfilenames.txt")
    #file_names = file.readlines()
    #file.close()

    #numberofjobs = 0
    #resto = len(file_names)%filesperjob
    #numberofjobs = len(file_names)/filesperjob
    #if (resto != 0):
    #    numberofjobs = numberofjobs + 1
        
    os.system("cp "+datfilename+".pevents "+label+"/.")
            
    #for f in xrange(0, len(file_names), filesperjob):
    #    numberofjobs += 1
    #    limits = (f, filesperjob+f)
    #    if (limits[1] > len(file_names)):
    #        limits = (f, len(file_names))
    #
    #    newdatafilename = "datafiles"+ str(numberofjobs) + ".dat"
    #    os.system("cp "+datfilename+".pevents process/"+newdatafilename+".pevents")
    #    
    #    out = open("process/" + newdatafilename, "w")
    #    
    #    inputdat = open("process/template.dat", "r")
    #    lines = inputdat.readlines()
    #    inputdat.close()
    #    
    #    for l in lines:
    #        if ("histfile" in l):
    #            l = l.replace(".root", "_"+str(numberofjobs)+".root")
    #            l = l.replace(".txt", "_"+str(numberofjobs)+".txt")
    #        out.write(l)
    #    out.write("\n") 
    #    
    #    for i in file_names[limits[0]:limits[1]]:
    #        out.write(i + "\n")
    #    out.write("\n")    
    #    out.close()
    
    os.system("cp -r AnalysisScripts/* "+label+"/.")
    thetarball = label+ timestamp + ".tgz"
    os.system("cd ../..;tar zcf " + thetarball + " src/"+basename+"/"+label+" src/"+basename+"/libLoopAll.so  src/EGamma src/CMGTools")
    
    if (not os.path.exists(hadoop_home + "/processed")):
        os.mkdir(hadoop_home + "/processed")
    
    if (not os.path.exists(hadoop_home + "/condorfiles")):
        os.mkdir(hadoop_home + "/condorfiles")
    
    os.system("cp ../../" + thetarball + " " + hadoop_home + "/condorfiles/")
    os.system("rm ../../" + thetarball)

    #for ijob in xrange(1, numberofjobs):
    #    newdatafilename = "datafiles" + str(ijob) + ".dat"
    #    os.system("rm -f " + newdatafilename)
    
    #os.system("rm -f process/allfilenames.txt")
    condordir = "/home/users/" + username + "/condorfiles/" + outputdir + "_pro"
    
    os.mkdir(hadoop_home + "/processed/" + outputdir)
    
    if (not os.path.exists("/home/users/" + username + "/condorfiles")):
        os.mkdir("/home/users/" + username + "/condorfiles")
        
    if (not os.path.exists(condordir)):
        os.mkdir(condordir)
    
    os.chdir(condordir)
    os.mkdir("output")
    
    for ijob in xrange(numberofjobs):
        jobname = label+"_job" + str(ijob)
        thesourcedir = username + timestamp + "job" + str(ijob)
        #newdatafilename = "datafiles" + str(ijob) + ".dat"
           
        file = open(jobname + ".bash", "w")
        file.write("#!/bin/bash\n")
        file.write("umask 002\n")
        file.write("\n")
        file.write("## setup env\n")
        file.write("export PATH=/usr/local/bin:/bin:/usr/bin\n")
        file.write("export SCRAM_ARCH=slc5_amd64_gcc462\n")
        file.write("source $OSG_APP/cmssoft/cms/cmsset_default.sh\n")
        file.write("scram project CMSSW " + CMSSWVersion + "\n")
        file.write("cd " + CMSSWVersion + "/src\n")
        file.write("eval `scram runtime -sh`\n")
        file.write("export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\n")
        file.write("export PYTHONPATH=${PYTHONPATH}\n")
        file.write("export CMSSW_SEARCH_PATH=${CMSSW_SEARCH_PATH}\n")
        file.write("export CMSSW_BASE=${CMSSW_BASE}\n")
        file.write("export CMSSW_SEARCH_PATH=$CMSSW_SEARCH_PATH:$CMSSW_BASE/CMGTools/External/data\n")
        file.write("export PATH=${PATH}\n")
        file.write("export SRM_PATH=$OSG_APP/ucsdt2/gLite32/d-cache/srm\n")
        file.write("export PATH=$PATH:$OSG_APP/ucsdt2/gLite32/d-cache/srm/bin\n")
        file.write("export JAVA_HOME=/usr/java/latest\n")
        file.write("landdir=\"$PWD\"\n")
        file.write("sourcedir=\"$PWD/%s\"\n" %(thesourcedir))
        file.write("\n")
        file.write("cd ..\n")
        file.write("cp %s/condorfiles/%s .\n" % (hadoop_home, thetarball))
        
        file.write("tar -zxf %s\n" % (thetarball))
        file.write("rm %s\n" % (thetarball) )
        file.write("cd src/\n")
        file.write("scram b -j 6\n")
        file.write("cd "+basename+"/"+label+"\n")
        file.write("mkdir ${sourcedir}\n")
        file.write("\n")
        
        file.write("./fitter.py -i %s -n %d -j %d \n" % (datfilename, numberofjobs, ijob))
        file.write("echo \"landir\"\n")
        file.write("echo ${landdir}\n")
        file.write("\n")
        file.write("ls -1 ${landdir}/"+basename+"/"+label+"/*.root > theoutputfiles.txt\n")
    
        file.write("\n")
        file.write("echo \"****************** catting theoutputfiles.txt ****************\"\n")
        file.write("cat theoutputfiles.txt\n")
        file.write("echo \"****************** catting theoutputfiles.txt ****************\"\n")
        file.write("\n")
    
        file.write("for ifile in `/usr/bin/less theoutputfiles.txt`;do\n")
        file.write("barefilename=`basename ${ifile}`\n")
        file.write("\n")
        file.write("srmcp -2 -debug=true -delegate=false file:////${ifile} %s%s/processed/%s/${barefilename} > srmcpfile.txt 2>&1 \n" % (hadoop_prefix, hadoop_home, outputdir))
        file.write("/usr/bin/less srmcpfile.txt\n")
        #file.write("\n")
        #file.write("if [ ! `grep -i fail srmcpfile.txt  | awk '{print NR}'| /usr/bin/tail -1` ];then \n")
        #file.write("    echo \"DONE\"\n")
        #file.write("else\n")
        #file.write("   echo \"srmcp failed - attempting globus-url-copy to home area of %s\"; \n" % (username))
        #file.write("   globus-url-copy file:///${ifile} gsiftp://uaf-2.t2.ucsd.edu/home/users/%s/${barefilename} > globusurlcopy.txt\n" % (username))
        #file.write("   if [ ! `less globusurlcopy.txt | awk '{print NR}'|tail -1` ]; then\n")
        #file.write("      echo \"DONE\"\n")
        #file.write("   else\n")
        #file.write("     echo \"globus-url-copy failed also - losing output, sorry - updating database with status 0\"; \n")
        #file.write("   fi\n")
        #file.write("fi\n")
        #file.write("\n")
        #file.write("\n")
        file.write("done\n")
        file.write("\n")
        file.write("cd ../../../..\n")
        file.write("echo \"clean up -------------------------------------\"\n")
        file.write("rm -rf " + CMSSWVersion + "\n")
        file.write("exit\n")
        file.close()
        
        file = open(jobname + ".sub", "w")
        text = """
    universe=grid
    Grid_Resource=gt2 osg-gw-2.t2.ucsd.edu:/jobmanager-condor
    executable=%s.bash
    stream_output = False
    stream_error  = False
    WhenToTransferOutput = ON_EXIT
    transfer_input_files =
    transfer_Output_files =
    log    = %s/test%s.log
    Notification = Never
    
    
    output = ./output/condor_batch.$(Cluster).$(Process).out
    error  = ./output/condor_batch.$(Cluster).$(Process).err
    queue
    """ % (jobname, condordir, str(ijob))
        file.write(text)
        file.close()
        
        print "\nsubmitting " + jobname + ".sub"
        os.system("condor_submit " + jobname + ".sub")
        time.sleep(1)
    
